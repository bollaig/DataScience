install.packages("rvest")
library(rvest)
library(rvest)
# Specifying url of website to be scrapped
url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
# Reading the HTML code from the website
web_page <- read_html(url)
# Quick look at the contents of web_page
head(web_page)
str(web_page)
# Using CSS selectors to scrap the “rankings” section - IMDB site is 
url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
# Reading the HTML code from the website
web_page <- read_html(url)
library(rvest)
# Specifying url of website to be scrapped
url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
# Reading the HTML code from the website
web_page <- read_html(url)
# Quick look at the contents of web_page
head(web_page)
str(web_page)
# Using CSS selectors to scrap the “rankings” section - IMDB site is 
# examined for this information first
rank_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > h3 > span.lister-item-index.unbold.text-primary')
head(rank_data_html, 10)
# should have 250 records
length(rank_data_html)
# Converting the ranking data from HTML to text
rank_data <- html_text(rank_data_html)
# Let's have a look at the rankings data
head(rank_data, 10)
# Data-Preprocessing: Converting rankings from string to numeric
rank_data <- as.numeric(rank_data)
# Let's have another look at the rankings data
head(rank_data, 10)
# Scrape the title section using the HTML data
# copied from "Copy selector"
title_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > h3 > a')
# Converting the title section data to text
title_data <- html_text(title_data_html)
# Let's have a look at the title
head(title_data, 10)
# Scrape the description section
description_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > p:nth-child(4)')
description_data <- html_text(description_data_html)
# Examine the description data
head(description_data, 10)
# Converting the description data to text
description_data <- html_text(description_data_html)
> head(description_data, 10)
# Tidy the decription data to remove the "\n" control character
# using gsub() function. Replaces all matches of a string
# with a string vector of same length
description_data <- gsub("\n", "", description_data)
head(description_data, 10)
# Alternatively we can scrape all of the text from the description section
# and then use gsub to remove unneeded portions of text
# Delete all text with punctuation included
# See http://www.endmemo.com/program/R/gsub.php for details
description_data <- gsub("[[:punct:]]", "", description_data)
# Delete all text with "Votes"
description_data <- gsub("Votes", "", description_data)
# Delete all text with "201*"
description_data <- gsub("201.*", "", description_data)
# Delete all text with "Gross"
description_data <- gsub("Gross", "", description_data)
# Delete all text with a number in it - eg PG12A
description_data <- gsub(".*[0-9]", "", description_data)
description_data <- gsub(".* min ", "", description_data)
description_data <- description_data[description_data == ""] <- NA
# Using CSS selectors to scrape the Movie runtime section
runtime_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > p:nth-child(2) > span.runtime')
# Converting the runtime data to text
runtime_data <- html_text(runtime_data_html)
# Let's have a look at the runtime
head(runtime_data, 10)
# "min" text inside running time values are going to be a problem
# Remove it using gsub function. And convert it using the as.numeric convertor
runtime_data <- gsub(" min", "", runtime_data)
runtime_data <- as.numeric(runtime_data)
#Let's have another look at the runtime data
head(runtime_data, 10)
# Using CSS selectors to scrape the Movie genre section
genre_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > p:nth-child(2) > span.genre')
# Converting the genre data to text
genre_data <- html_text(genre_data_html)
# Let's have a look at the genre data
head(genre_data, 10)
# We need to tidy the genre data to remove "\n" control character
# and to remove additional spaces after each ",“ Do this using gsub() function
genre_data <- gsub("\n", "", genre_data)
# Data-Preprocessing: removing excess spaces found at the end of the genre data
genre_data <- gsub(" ", "", genre_data)
head(genre_data, 10)
# Using CSS selectors to scrape the Movie genre section
genre_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > p:nth-child(2) > span.genre')
# Converting the genre data to text
genre_data <- html_text(genre_data_html)
# Let's have a look at the genre data
head(genre_data, 10)
# We need to tidy the genre data to remove "\n" control character
# and to remove additional spaces after each ",“ Do this using gsub() function
genre_data <- gsub("\n", "", genre_data)
# Data-Preprocessing: removing excess spaces found at the end of the genre data
genre_data <- gsub(" ", "", genre_data)
head(genre_data, 10)
# There are multiple genres for each film.
# We only need to have the first genre so we can use gsub() and a wildcard to remove
# all text after the first comma
genre_data <- gsub(",.*", "", genre_data)
# Now let's examine the genre_data
head(genre_data, 10)
# Scraping the IMDB rating section
rating_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > div > div.inline-block.ratings-imdb-rating > strong')
# Converting the ratings data to text
rating_data <- html_text(rating_data_html)
# Data-Preprocessing: converting ratings to numerical values
rating_data <- as.numeric(rating_data)
# Let's have another look at the ratings data
head(rating_data, 10)
length(rating_data)
# Scraping the directors section
directors_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > p:nth-child(5)')
#Converting the directors data to text
directors_data <- html_text(directors_data_html)
#Let's have a look at the directors data
head(directors_data, 10)
#Data-Preprocessing: converting directors data into factors
directors_data <- as.factor(directors_data)
# Check the length of the directors data
length(directors_data)
# Scraping the actors section
actors_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > p:nth-child(5) > a:nth-child(3)')
# Converting the gross actors data to text
actors_data <- html_text(actors_data_html)
# Let's have a look at the actors data
head(actors_data, 10)
# Data-Preprocessing: convert the actors data into factors
actors_data <- as.factor(actors_data)
length(actors_data)
# Scrape the metascore section
metascore_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > div > div.inline-block.ratings-metascore > span')
#Converting the data to text
metascore_data <- html_text(metascore_data_html)
#Let's have a look at the metascore 
head(metascore_data, 10)
# Problem with the metascore data length
length(metascore_data)
# Data-Preprocessing: removing extra space in metascore data
metascore_data <- gsub(" ", "", metascore_data)
head(metascore_data, 100)
# Data should contain 250 values (50 movies)
length(metascore_data)
summary(metascore_data)
# Through analysis, this data is missing
missing_data <- c(1, 4, 6, 10, 16, 17, 23, 25, 29, 33, 37, 43, 47, 49, 50, 52, 57:59, 64:66, 73, 74, 77, 78, 83, 85, 87, 89:91, 94, 95, 97, 100, 103, 105112, 113, 115, 117, 118, 120:122, 130, 136:138, 141, 147, 149, 152, 158:160, 162, 164:166, 169, 170, 173, 178, 183, 184, 189, 193, 197:199, 205:207, 209:212, 220:222, 225, 227, 228, 231, 233, 235, 240, 241, 247, 249)
length(missing_data)
metascore_data <- replace_missing_data(metascore_data, missing_data)
head(metascore_data, 10)
str(metascore_data)
# Scrape the gross revenue section
gross_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > p.sort-num_votes-visible > span:nth-child(5)')
#Converting the gross revenue data to text
gross_data <- html_text(gross_data_html)
#Let's have a look at the gross data
head(gross_data, 10)
length(gross_data)
# Data-Preprocessing: removing 'M' sign
gross_data <- gsub("M", "", gross_data)
head(gross_data, 10)
# Data-Preprocessing: removing '$' sign with substring
gross_data <- substring(gross_data, 2, 7)
#Let's check the length of gross data
# Should be 50 - lots of data missing
length(gross_data)
summary(gross_data)
# Data-Preprocessing: converting gross to numerical
gross_data <- as.numeric(gross_data)
# Now lets combine all of the vectors into a data frame
movies <- data.frame(rank = rank_data, Title = title_data, Desc = description_data, Genre = genre_data, Runtime = runtime_data)
head(movies, 60)
length(description_data)
summary(movies)
length(rank_data)
length(title_data)
length(description_data)
length(genre_data)
length(runtime_data)
description_data_html
description_data <- html_text(description_data_html)
# Examine the description data
head(description_data, 10)
length(description_data)
length(runtime_data_html)
runtime_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > p:nth-child(2) > span.runtime')
length(runtime_data_html)
title_data
runtime_data
length(rank_data)
length(title_data)
length(description_data)
length(genre_data)
length(runtime_data)
movies <- data.frame(rank = rank_data, Title = title_data, Desc = description_data, Genre = genre_data, Runtime = runtime_data)
runtime_data(50) <- 0
runtime_data[50] <- 0
length(runtime_data)
movies <- data.frame(rank = rank_data, Title = title_data, Desc = description_data, Genre = genre_data, Runtime = runtime_data)
head(movies, 60)
length(description_data)
summary(movies)
str(movies)
movies
library(rvest)
# Specifying url of website to be scrapped
#url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
url <- 'http://www.cso.ie/en/census/census2016reports/census2016smallareapopulationstatistics/'
# Reading the HTML code from the website
web_page <- read_html(url)
# Quick look at the contents of web_page
head(web_page)
str(web_page)
rank_data_html <- html_nodes(web_page, '#columnLeft > div > div > div > div.moduleBody > ul > script:nth-child(1)')
head(rank_data_html, 10)
# should have 250 records
length(rank_data_html)
rank_data_html <- html_nodes(web_page, '#body > div.container.content > div.row.mainContent > columnLeft > div > div > div > div.moduleBody > ul > script:nth-child(1)')
head(rank_data_html, 10)
# should have 250 records
length(rank_data_html)
head(rank_data_html, 10)
url <- 'https://en.wikipedia.org/wiki/List_of_countries_by_number_of_Internet_users'
# Reading the HTML code from the website
web_page <- read_html(url)
# Quick look at the contents of web_page
head(web_page)
str(web_page)
#rank_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > h3 > span.lister-item-index.unbold.text-primary')
rank_data_html <- html_nodes(web_page, '##mw-content-text > div > table.wikitable.sortable.jquery-tablesorter > tbody > tr:nth-child(1) > td:nth-child(1) > a')
head(rank_data_html, 10)
 # should have 250 records
rank_data_html <- html_nodes(web_page, '#mw-content-text > div > table.wikitable.sortable.jquery-tablesorter > tbody > tr:nth-child(1) > td:nth-child(1) > a')
head(rank_data_html, 10)
 # should have 250 records
length(rank_data_html)
rank_data_html <- html_nodes(web_page, '#mw-content-text > div > table.wikitable.sortable.jquery-tablesorter > tbody > tr:nth-child(1)')
head(rank_data_html, 10)
 # should have 250 records
length(rank_data_html)
rank_data_html <- html_nodes(web_page, '#mw-content-text > div > table.wikitable.sortable.jquery-tablesorter > tbody > tr:nth-child(1) > td:nth-child(1)')
head(rank_data_html, 10)
 # should have 250 records
length(rank_data_html)
rank_data_html <- html_nodes(web_page, '#mw-content-text > div > table.wikitable.sortable.jquery-tablesorter > tbody > tr:nth-child(1) > td:nth-child(1) > a')
head(rank_data_html, 10)
 # should have 250 records
length(rank_data_html)
# Converting the ranking data from HTML to text
rank_data_html <- html_nodes(web_page, '#mw-content-text > div > table.wikitable.sortable.jquery-tablesorter > tbody > tr:nth-child(n) > td:nth-child(1) > a')
head(rank_data_html, 10)
 # should have 250 records
length(rank_data_html)
url <- 'https://en.wikipedia.org/wiki/List_of_countries_by_number_of_Internet_users'
# Reading the HTML code from the website
web_page <- read_html(url)
# Quick look at the contents of web_page
head(web_page)
str(web_page)
# Using CSS selectors to scrap the “rankings” section - IMDB site is 
# examined for this information first
#rank_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > h3 > span.lister-item-index.unbold.text-primary')
rank_data_html <- html_nodes(web_page, '#mw-content-text > div > table.wikitable.sortable.jquery-tablesorter > tbody > tr:nth-child(n) > td:nth-child(1) > a')
head(rank_data_html, 10)
 # should have 250 records
length(rank_data_html)
rank_data_html <- html_nodes(web_page, '#mw-content-text > div > table.wikitable.sortable.jquery-tablesorter > tbody > tr:nth-child(1) > td:nth-child(n) > a')
head(rank_data_html, 10)
 # should have 250 records
length(rank_data_html)
url <- 'https://en.wikipedia.org/wiki/List_of_Irish_counties_by_population'
# Reading the HTML code from the website
web_page <- read_html(url)
# Quick look at the contents of web_page
head(web_page)
str(web_page)
# Using CSS selectors to scrap the “rankings” section - IMDB site is 
# examined for this information first
#rank_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > h3 > span.lister-item-index.unbold.text-primary')
rank_data_html <- html_nodes(web_page, '#mw-content-text > div > table > tbody > tr:nth-child(1) > td:nth-child(2) > a')
head(rank_data_html, 10)
 # should have 250 records
length(rank_data_html)
# Converting the ranking data from HTML to text
rank_data_html <- html_nodes(web_page, '#mw-content-text > div > table > tbody > tr:nth-child(n) > td:nth-child(2) > a')
head(rank_data_html, 10)
 # should have 250 records
length(rank_data_html)
url <- 'https://www.citypopulation.de/php/ireland.php'
# Reading the HTML code from the website
web_page <- read_html(url)
# Quick look at the contents of web_page
head(web_page)
str(web_page)
>  # examined for this information first #rank_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > h3 > span.lister-item-index.unbold.text-primary') rank_data_html <- html_nodes(web_page, '#iCW > span > a')
rank_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n)')
head(rank_data_html, 10)
 # should have 250 records
length(rank_data_html)
location_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n)')
head(location_data_html, 10)
 # should have 250 records
length(location_data_html)
# Converting the ranking data from HTML to text
location_data <- html_text(location_data_html)
# Let's have a look at the rankings data
head(location_data, 10)
p2016_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
head(p2016_data_html)
lenght(p2016_data_html)
length(p2016_data_html)
p2016_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p2016_data <- html_text(p2016_data_html)
head(p2016_data)
length(p2016_data)
p2016_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p2016_data <- html_text(p2016_data_html)
p2016_data <- as.numeric(p2016)
head(p2016_data)
length(p2016_data)
p2016_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p2016_data <- html_text(p2016_data_html)
p2016_data <- as.numeric(p2016_data)
head(p2016_data)
length(p2016_data)
p2016_data <- html_text(p2016_data_html)
head(p2016_data)
length(p2016_data)
class(p2016_data)
p2016_data <= gsub(",", "", p2016_data)
head(p2016_data)
p2016_data <- as.numeric(p2016_data)
p2016_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p2016_data <- html_text(p2016_data_html)
p2016_data <= gsub(",", "", p2016_data)
head(p2016_data)
test <- 'noel'
test <- gsub('o', '', test)
test
p2016_data <- html_text(p2016_data_html)
p2016_data <= gsub(',', '', p2016_data)
head(p2016_data)
length(p2016_data)
class(p2016_data)
test <- '40,942'
test <- gsub(',', '', test)
test
test <- '40,942'
test
test <- gsub(',', '', test)
test
p2016_data <= sub(',', '', p2016_data)
p2016_data <- as.numeric(p2016_data)
p2016_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p2016_data <- html_text(p2016_data_html)
p2016_data <= sub('4', '9', p2016_data)
head(p2016_data)
p2016_data <- sub('4', '9', p2016_data)
p2016_data <- as.numeric(p2016_data)
head(p2016_data)
p2016_data <- html_text(p2016_data_html)
p2016_data <- sub('4', '9', p2016_data)
head(p2016_data)
p2016_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p2016_data <- html_text(p2016_data_html)
p2016_data <- sub(',', '', p2016_data)
p2016_data <- as.numeric(p2016_data)
head(p2016_data)
p1991_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio1')
p1991_data <- html_text(p1991_data_html)
p1991_data <- sub(',', '', p1991_data)
p1991_data <- as.numeric(p1991_data)
head(p1991_data)
length(p1991_data)
p1991_data
p1991_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p1991_data <- html_text(p1991_data_html)
p1991_data
p1991_data <- sub(',', '', p1991_data)
p1991_data <- as.numeric(p1991_data)
head(p1991_data)
length(p1991_data)
p1996_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio5')
p1996_data <- html_text(p1996_data_html)
p1996_data <- sub(',', '', p1996_data)
p1996_data <- as.numeric(p1996_data)
head(p1996_data)
length(p1996_data)
p2002_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio4')
p2002_data <- html_text(p2002_data_html)
p2002_data <- sub(',', '', p2002_data)
p2002_data <- as.numeric(p2002_data)
head(p2002_data)
length(p2002_data)
p2006_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio3')
p2006_data <- html_text(p2006_data_html)
p2006_data <- sub(',', '', p2006_data)
p2006_data <- as.numeric(p2006_data)
head(p2006_data)
length(p2006_data)
p2011_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio2')
p2011_data <- html_text(p2011_data_html)
p2011_data <- sub(',', '', p2011_data)
p2011_data <- as.numeric(p2011_data)
head(p2011_data)
length(p2011_data)
p2016_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio1')
p2016_data <- html_text(p2016_data_html)
p2016_data <- sub(',', '', p2016_data)
p2016_data <- as.numeric(p2016_data)
head(p2016_data)
length(p2016_data)
# Data-Preprocessing: Converting rankings from string to numeric
area_status_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(1) > td.rstatus')
area_status_data <- html_text(area_status_data_html)
area_status_data
area_status_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rstatus')
area_status_data <- html_text(area_status_data_html)
area_status_data
area_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rname')
area_data <- html_text(area_data_html)
area_data
pop_stats <- data.frame(area = area_data, status = area_status_data, 1996 = p1996_data, 2002 = p2002_data, 2006 = p2006_data, 2011 = p2011_data, 2016 = p2016_data,)
pop_stats <- data.frame(area = area_data, status = area_status_data, p1996 = p1996_data, p2002 = p2002_data, p2006 = p2006_data, p2011 = p2011_data, p2016 = p2016_data,)
pop_stats <- data.frame(area = area_data, status = area_status_data, p1996 = p1996_data, p2002 = p2002_data, p2006 = p2006_data, p2011 = p2011_data, p2016 = p2016_data)
head(pop_stats, 60)
summary(pop_stats)
str(pop_stats)
pop_stats
head(web_page)
str(web_page)
p1991_total <- pop_stats[, p1991]
p1991_total
url <- 'https://www.citypopulation.de/php/ireland.php'
# Reading the HTML code from the website
web_page <- read_html(url)
p1991_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p1996_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio5')
p2002_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio4')
p2006_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio3')
p2011_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio2')
p2016_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio1')
area_status_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rstatus')
area_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rname')
#Extract text from html
p1991_data <- html_text(p1991_data_html)
p1996_data <- html_text(p1996_data_html)
p2002_data <- html_text(p2002_data_html)
p2006_data <- html_text(p2006_data_html)
p2011_data <- html_text(p2011_data_html)
p2016_data <- html_text(p2016_data_html)
area_status_data <- html_text(area_status_data_html)
area_data <- html_text(area_data_html)
#Remove the thousands separator
p1991_data <- sub(',', '', p1991_data)
p1996_data <- sub(',', '', p1996_data)
p2002_data <- sub(',', '', p2002_data)
p2006_data <- sub(',', '', p2006_data)
p2011_data <- sub(',', '', p2011_data)
p2016_data <- sub(',', '', p2016_data)
#Convert to integers
p1991_data <- as.numeric(p1991_data)
p1996_data <- as.numeric(p1996_data)
p2002_data <- as.numeric(p2002_data)
p2006_data <- as.numeric(p2006_data)
p2011_data <- as.numeric(p2011_data)
p2016_data <- as.numeric(p2016_data)
#Load a data frame
pop_stats <- data.frame(area = area_data, status = area_status_data, ,p1991 = p1991_data, p1996 = p1996_data, p2002 = p2002_data, p2006 = p2006_data, p2011 = p2011_data, p2016 = p2016_data)
head(pop_stats, 60)
#Display data frame information
summary(pop_stats)
pop_stats <- data.frame(area = area_data, status = area_status_data, p1991 = p1991_data, p1996 = p1996_data, p2002 = p2002_data, p2006 = p2006_data, p2011 = p2011_data, p2016 = p2016_data)
head(pop_stats, 60)
#Display data frame information
summary(pop_stats)
str(pop_stats)
pop_stats
p1991_total <- pop_stats[, p1991]
p1991_total <- pop_stats$p1991
p1991_total
p1991_total <- sum(pop_stats$p1991)
p1991_total
totals <- c(area_total, status_total, p1991_total, p1996_total, p2002_total, p2006_total, p2011_total, p2016_total)
p1991_total <- sum(pop_stats$p1991)
p2002_total <- sum(pop_stats$p2002)
p2006_total <- sum(pop_stats$p2006)
p2011_total <- sum(pop_stats$p2011)
p2016_total <- sum(pop_stats$p2016)
area_total <- 'Totals'
status_total < NA
totals <- c(area_total, status_total, p1991_total, p1996_total, p2002_total, p2006_total, p2011_total, p2016_total)
status_total <- NA
totals <- c(area_total, status_total, p1991_total, p1996_total, p2002_total, p2006_total, p2011_total, p2016_total)
p1996_total <- sum(pop_stats$p1996)
totals <- c(area_total, status_total, p1991_total, p1996_total, p2002_total, p2006_total, p2011_total, p2016_total)
totals
pop_stats <- rbind(pop_stats, totals)
pop_stats <- rbind(pop_stats, totals)
area_total <- NA
totals <- c(area_total, status_total, p1991_total, p1996_total, p2002_total, p2006_total, p2011_total, p2016_total)
totals
area_total <- "Totals"
totals <- c(area_total, status_total, p1991_total, p1996_total, p2002_total, p2006_total, p2011_total, p2016_total)
totals
pop_stats <- rbind(pop_stats, totals)
pop_stats
area_total <- c("Totals")
totals <- c(area_total, status_total, p1991_total, p1996_total, p2002_total, p2006_total, p2011_total, p2016_total)
totals
pop_stats <- rbind(pop_stats, totals)
p1991_total <- sum(pop_stats$p1991)
p1996_total <- sum(pop_stats$p1996)
p2002_total <- sum(pop_stats$p2002)
p2006_total <- sum(pop_stats$p2006)
p2011_total <- sum(pop_stats$p2011)
p2016_total <- sum(pop_stats$p2016)
library(rvest)
# Specifying url of website to be scrapped
url <- 'https://www.citypopulation.de/php/ireland.php'
# Reading the HTML code from the website
web_page <- read_html(url)
#rank_data_html <- html_nodes(web_page, '#main > div > div > div.lister-list > div:nth-child(n) > div.lister-item-content > h3 > span.lister-item-index.unbold.text-primary')
#location_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n)')
#head(location_data_html, 10)
 # should have 250 records
#length(location_data_html)
# Converting the ranking data from HTML to text
#location_data <- html_text(location_data_html)
# Let's have a look at the rankings data
#head(location_data, 10)
#Extract html from website
p1991_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p1996_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio5')
p2002_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio4')
p2006_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio3')
p2011_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio2')
p2016_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio1')
area_status_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rstatus')
area_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rname')
#Extract text from html
p1991_data <- html_text(p1991_data_html)
p1996_data <- html_text(p1996_data_html)
p2002_data <- html_text(p2002_data_html)
p2006_data <- html_text(p2006_data_html)
p2011_data <- html_text(p2011_data_html)
p2016_data <- html_text(p2016_data_html)
area_status_data <- html_text(area_status_data_html)
area_data <- html_text(area_data_html)
#Remove the thousands separator
p1991_data <- sub(',', '', p1991_data)
p1996_data <- sub(',', '', p1996_data)
p2002_data <- sub(',', '', p2002_data)
p2006_data <- sub(',', '', p2006_data)
p2011_data <- sub(',', '', p2011_data)
p2016_data <- sub(',', '', p2016_data)
#Convert to integers
p1991_data <- as.numeric(p1991_data)
p1996_data <- as.numeric(p1996_data)
p2002_data <- as.numeric(p2002_data)
p2006_data <- as.numeric(p2006_data)
p2011_data <- as.numeric(p2011_data)
p2016_data <- as.numeric(p2016_data)
#Load a data frame
pop_stats <- data.frame(area = area_data, status = area_status_data, p1991 = p1991_data, p1996 = p1996_data, p2002 = p2002_data, p2006 = p2006_data, p2011 = p2011_data, p2016 = p2016_data)
head(pop_stats, 60)
#Display data frame information
summary(pop_stats)
str(pop_stats)
pop_stats
p1991_total <- sum(pop_stats$p1991)
p1996_total <- sum(pop_stats$p1996)
p2002_total <- sum(pop_stats$p2002)
p2006_total <- sum(pop_stats$p2006)
p2011_total <- sum(pop_stats$p2011)
p2016_total <- sum(pop_stats$p2016)
totals <- c(p1991_total, p1996_total, p2002_total, p2006_total, p2011_total, p2016_total)
totals
plot(totals, xlab='this is x', ylab='this is y')
plot(totals~x-axis, xlab='this is x', ylab='this is y')
plot(totals~x-axis, xlab='this is x', ylab='this is y')
plot(totals, xlab = 'this is x', ylab = 'this is y')
axis(1, at = 1:10, labels = letters[1:10])
plot(totals, xlab = 'this is x', ylab = 'this is y', labels=x-axis)
plot(totals, xlab = 'this is x', ylab = 'this is y', labels=x-axis)
plot(totals, xlab = 'this is x', ylab = 'this is y', labels=x_axis)
plot(totals, xlab = 'this is x', ylab = 'this is y', labels = xaxis)
plot(totals, xlab = 'this is x', ylab = 'this is y', labels = "1991", "1666")
xaxis <- c('1991', '1996', '2002', '2006', '2011', '2016')
plot(totals, xlab = 'this is x', ylab = 'this is y', labels = xaxis)
plot(totals, xlab = 'this is x', ylab = 'this is y', axis(labels = xaxis))
axis(labels = xaxis)
plot(totals, xlab = 'this is x', ylab = 'this is y')
axis(labels = xaxis)
plot(totals, xlab = 'this is x', ylab = 'this is y')
axis(1, at = 1:6, labels = xaxis)
plot(totals, xlab = 'this is x', ylab = 'this is y', axis(1, at = 1:6, labels = xaxis))
plot(totals, xlab = 'this is x', ylab = 'this is y', axis(1, at = 1:6, labels = xaxis))
plot(totals, xlab = 'this is x', ylab = 'this is y')
plot(totals, xlab = 'this is x', ylab = 'this is y', xact = "n")
axis(1, at = 1:6, labels = xaxis)
plot(totals, xlab = 'this is x', ylab = 'this is y', xaxt = "n")
axis(1, at = 1:6, labels = xaxis)
plot(totals, xlab = 'this is x', ylab = 'this is y', axis(1, at = 1:6, labels = xaxis))
plot(totals, xlab = 'this is x', axis(1, at = 1:6, labels = xaxis), ylab = 'this is y')
plot(totals, xlab = 'this is x', axis(1, at = 1:6, labels = xaxis), ylab = 'this is y')
plot(totals, xlab = 'this is x', axis(2, at = 1:6, labels = xaxis), ylab = 'this is y')
plot(totals, xlab = 'this is x', axis(2, at = 1:6, labels = xaxis), ylab = 'this is y')
plot(totals, xlab = 'this is x', axis(1, at = 2:6, labels = xaxis), ylab = 'this is y')
plot(totals, xlab = 'this is x', axis(1, at = 2:6, labels = xaxis), ylab = 'this is y')
plot(totals, type = 'b'm xlab = 'this is x', ylab = 'this is y', xaxt = "n")
plot(totals, type = 'b', xlab = 'this is x', ylab = 'this is y', xaxt = "n")
axis(1, at = 1:6, labels = xaxis)
plot(totals, type = 'b', xlab = 'Census Year', ylab = 'Population', xaxt = "n")
axis(1, at = 1:6, labels = xaxis)
library(rvest)
# Specify the url of the citypopulation website
url <- 'https://www.citypopulation.de/php/ireland.php'
# Read the HTML code from the website
web_page <- read_html(url)
#Extract html from website
area_data_html   <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rname')
status_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rstatus')
p1991_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p1996_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio5')
p2002_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio4')
p2006_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio3')
p2011_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio2')
p2016_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio1')
#Extract text from html
area_data   <- html_text(area_data_html)
status_data <- html_text(status_data_html)
p1991_data  <- html_text(p1991_data_html)
p1996_data  <- html_text(p1996_data_html)
p2002_data  <- html_text(p2002_data_html)
p2006_data  <- html_text(p2006_data_html)
p2011_data  <- html_text(p2011_data_html)
p2016_data  <- html_text(p2016_data_html)
#Remove the thousands separator
p1991_data <- sub(',', '', p1991_data)
p1996_data <- sub(',', '', p1996_data)
p2002_data <- sub(',', '', p2002_data)
p2006_data <- sub(',', '', p2006_data)
p2011_data <- sub(',', '', p2011_data)
p2016_data <- sub(',', '', p2016_data)
#Convert to integers
p1991_data <- as.numeric(p1991_data)
p1996_data <- as.numeric(p1996_data)
p2002_data <- as.numeric(p2002_data)
p2006_data <- as.numeric(p2006_data)
p2011_data <- as.numeric(p2011_data)
p2016_data <- as.numeric(p2016_data)
#Load a data frame
pop_stats <- data.frame(area = area_data, status = status_data, p1991  = p1991_data,                                                                 p1996  = p1996_data,                                                                 p2002  = p2002_data,                                                                 p2006  = p2006_data,                                                                 p2011  = p2011_data,                                                                 p2016  = p2016_data)
head(pop_stats, 60)
#Display data frame information
summary(pop_stats)
str(pop_stats)
pop_stats
#Generate totals
p1991_total <- sum(pop_stats$p1991)
p1996_total <- sum(pop_stats$p1996)
p2002_total <- sum(pop_stats$p2002)
p2006_total <- sum(pop_stats$p2006)
p2011_total <- sum(pop_stats$p2011)
p2016_total <- sum(pop_stats$p2016)
#Show population trend
xaxis <- c('1991', '1996', '2002', '2006', '2011', '2016')
totals <- c(p1991_total, p1996_total, p2002_total, p2006_total, p2011_total, p2016_total)
plot(totals, type = 'b', xlab = 'Census Year', ylab = 'Population', xaxt = "n")
axis(1, at = 1:6, labels = xaxis)
web_page
area_data_html   <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rname')
status_data_html <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rstatus')
p1991_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio6')
p1996_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio5')
p2002_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio4')
p2006_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio3')
p2011_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio2')
p2016_data_html  <- html_nodes(web_page, '#tl > tbody > tr:nth-child(n) > td.rpop.prio1')
#Extract text from html
area_data   <- html_text(area_data_html)
status_data <- html_text(status_data_html)
p1991_data  <- html_text(p1991_data_html)
p1996_data  <- html_text(p1996_data_html)
p2002_data  <- html_text(p2002_data_html)
p2006_data  <- html_text(p2006_data_html)
p2011_data  <- html_text(p2011_data_html)
p2016_data  <- html_text(p2016_data_html)
#Remove the thousands separator
p1991_data <- gsub(',', '', p1991_data)
p1996_data <- gsub(',', '', p1996_data)
p2002_data <- gsub(',', '', p2002_data)
p2006_data <- gsub(',', '', p2006_data)
p2011_data <- gsub(',', '', p2011_data)
p2016_data <- gsub(',', '', p2016_data)
#Convert to integers
p1991_data <- as.numeric(p1991_data)
p1996_data <- as.numeric(p1996_data)
p2002_data <- as.numeric(p2002_data)
p2006_data <- as.numeric(p2006_data)
p2011_data <- as.numeric(p2011_data)
p2016_data <- as.numeric(p2016_data)
#Load a data frame
pop_stats <- data.frame(area = area_data, status = status_data, p1991  = p1991_data,                                                                 p1996  = p1996_data,                                                                 p2002  = p2002_data,                                                                 p2006  = p2006_data,                                                                 p2011  = p2011_data,                                                                 p2016  = p2016_data)
head(pop_stats)
#Display data frame information
summary(pop_stats)
str(pop_stats)
pop_stats
#Generate totals
p1991_total <- sum(pop_stats$p1991)
p1996_total <- sum(pop_stats$p1996)
p2002_total <- sum(pop_stats$p2002)
p2006_total <- sum(pop_stats$p2006)
p2011_total <- sum(pop_stats$p2011)
p2016_total <- sum(pop_stats$p2016)
#Show population trend
xaxis <- c('1991', '1996', '2002', '2006', '2011', '2016')
totals <- c(p1991_total, p1996_total, p2002_total, p2006_total, p2011_total, p2016_total)
plot(totals, type = 'b', xlab = 'Census Year', ylab = 'Population', xaxt = "n")
axis(1, at = 1:6, labels = xaxis)
area_data
area_data_html
head(area_data_html)
head(area_data)
p1991_data  <- html_text(p1991_data_html)
head(p1991_data)
#Remove the thousands separator
p1991_data <- gsub(',', '', p1991_data)
head(p1991_data)
p1991_data <- as.numeric(p1991_data)
head(p1991_data)
class(web_page)
web_page
str(web_page)
str(area_data_html)
str(area_data)
